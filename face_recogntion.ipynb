{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import glob, json\n",
    "import numpy as np\n",
    "import faiss, glob, os\n",
    "import tensorflow as tf\n",
    "from deepface import DeepFace\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        \"VGG-Face\", \n",
    "        \"Facenet\", \n",
    "        \"Facenet512\", \n",
    "        \"OpenFace\", \n",
    "        \"DeepFace\", \n",
    "        \"DeepID\", \n",
    "        \"ArcFace\", \n",
    "        \"Dlib\", \n",
    "        \"SFace\",\n",
    "        \"GhostFaceNet\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "                data_dir = \"data/facedb/*/*.*\",\n",
    "                json_dir = \"data/facedb_jsons/{}/{}\",\n",
    "                ):\n",
    "    for img_path in glob.glob(data_dir):\n",
    "        person_objs = []\n",
    "        img_path = img_path.replace(\"\\\\\", \"/\")\n",
    "        person_name = img_path.split(\"/\")[-2]\n",
    "        file_name = img_path.split(\"/\")[-1]\n",
    "        face_objs = DeepFace.represent(\n",
    "                                    img_path = img_path,\n",
    "                                    model_name = \"Facenet512\",\n",
    "                                    enforce_detection = False\n",
    "                                    )\n",
    "        if len(face_objs) > 0:\n",
    "            for i in range(len(face_objs)):\n",
    "                person_json = {}\n",
    "                facial_area = face_objs[i]['facial_area']\n",
    "                x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "                person_json['person_name'] = person_name\n",
    "                person_json['facial_area'] = facial_area\n",
    "                person_json['file_name'] = file_name\n",
    "                person_json['coor'] = (x, y, w, h)\n",
    "                person_objs.append(person_json)\n",
    "\n",
    "        if len(person_objs) > 0:\n",
    "            os.makedirs(json_dir.format(person_name, \"\"), exist_ok=True)\n",
    "            json_file = json_dir.format(person_name, file_name.replace(\".jpg\", \".json\"))\n",
    "            with open(json_file, 'w') as json_file:\n",
    "                json.dump(person_objs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detection_dataset(\n",
    "                            data_dir = \"data/facedb/*/*.*\"\n",
    "                            ):\n",
    "    X, Y = [], []\n",
    "    for img_path in glob.glob(data_dir):\n",
    "        json_path = img_path.replace(\"\\\\\", \"/\").replace(\"facedb\", \"facedb_jsons\").replace(\".jpg\", \".json\")\n",
    "        if (os.path.exists(json_path)):\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                person = json_data[0]\n",
    "                x, y, w, h = person['coor']\n",
    "                img = cv.imread(img_path)\n",
    "                img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "                img = cv.resize(img, (128, 128))\n",
    "\n",
    "                X.append(img)\n",
    "                Y.append((x, y, w, h))\n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape:  (412, 128, 128, 3)\n",
      "Label Shape:  (412, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_detection_dataset() \n",
    "\n",
    "print(\"Image Shape: \", X.shape)\n",
    "print(\"Label Shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - loss: 340148.0000 - mae: 430.4422\n",
      "Epoch 2/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - loss: 128901.2969 - mae: 251.5449\n",
      "Epoch 3/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - loss: 83210.1328 - mae: 206.2079\n",
      "Epoch 4/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - loss: 86851.2188 - mae: 201.2630\n",
      "Epoch 5/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - loss: 80012.8203 - mae: 187.8143\n",
      "Epoch 6/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - loss: 61915.4297 - mae: 174.3071\n",
      "Epoch 7/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - loss: 61768.9883 - mae: 171.3458\n",
      "Epoch 8/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - loss: 67279.4375 - mae: 178.7769\n",
      "Epoch 9/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - loss: 55396.8086 - mae: 162.4477\n",
      "Epoch 10/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - loss: 43442.3789 - mae: 142.8891\n",
      "Epoch 11/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 178ms/step - loss: 38480.3320 - mae: 139.1831\n",
      "Epoch 12/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - loss: 29002.2559 - mae: 119.4809\n",
      "Epoch 13/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - loss: 24687.0312 - mae: 113.7551\n",
      "Epoch 14/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 211ms/step - loss: 17105.6895 - mae: 93.9770\n",
      "Epoch 15/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 213ms/step - loss: 17361.5020 - mae: 93.9684\n",
      "Epoch 16/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 190ms/step - loss: 11227.8809 - mae: 80.9096\n",
      "Epoch 17/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 184ms/step - loss: 8629.4053 - mae: 69.0511\n",
      "Epoch 18/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 178ms/step - loss: 5435.0786 - mae: 55.5739\n",
      "Epoch 19/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step - loss: 3452.7039 - mae: 43.0383\n",
      "Epoch 20/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - loss: 3122.8342 - mae: 41.8955\n",
      "Epoch 21/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - loss: 2378.4988 - mae: 37.9150\n",
      "Epoch 22/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 214ms/step - loss: 1869.6038 - mae: 33.6619\n",
      "Epoch 23/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - loss: 2384.1584 - mae: 36.3008\n",
      "Epoch 24/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 190ms/step - loss: 2252.8787 - mae: 35.4516\n",
      "Epoch 25/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - loss: 2274.6331 - mae: 34.3616\n",
      "Epoch 26/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 178ms/step - loss: 2195.2646 - mae: 35.6701\n",
      "Epoch 27/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - loss: 1972.5297 - mae: 32.8034\n",
      "Epoch 28/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 181ms/step - loss: 2005.2480 - mae: 32.6225\n",
      "Epoch 29/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step - loss: 1339.8267 - mae: 26.7550\n",
      "Epoch 30/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 179ms/step - loss: 1140.4050 - mae: 24.3216\n",
      "Epoch 31/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - loss: 987.8428 - mae: 23.3498\n",
      "Epoch 32/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - loss: 672.6849 - mae: 18.7747\n",
      "Epoch 33/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 194ms/step - loss: 576.9048 - mae: 17.0077\n",
      "Epoch 34/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - loss: 418.0328 - mae: 14.6422\n",
      "Epoch 35/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - loss: 273.1869 - mae: 12.1585\n",
      "Epoch 36/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - loss: 194.0654 - mae: 10.5479\n",
      "Epoch 37/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 187ms/step - loss: 146.7552 - mae: 9.0193\n",
      "Epoch 38/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - loss: 91.2465 - mae: 6.9086\n",
      "Epoch 39/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 191ms/step - loss: 63.3270 - mae: 5.9529\n",
      "Epoch 40/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step - loss: 59.3033 - mae: 5.7437\n",
      "Epoch 41/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 248ms/step - loss: 56.3940 - mae: 5.6230\n",
      "Epoch 42/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 37.4060 - mae: 4.5464\n",
      "Epoch 43/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 32.1007 - mae: 4.1506\n",
      "Epoch 44/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 254ms/step - loss: 29.4563 - mae: 3.9660\n",
      "Epoch 45/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 282ms/step - loss: 26.2775 - mae: 3.8679\n",
      "Epoch 46/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 267ms/step - loss: 19.0535 - mae: 3.2604\n",
      "Epoch 47/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - loss: 24.6974 - mae: 3.5160\n",
      "Epoch 48/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 16.5806 - mae: 2.8849\n",
      "Epoch 49/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 16.4942 - mae: 3.0035\n",
      "Epoch 50/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - loss: 17.4596 - mae: 3.0447\n",
      "Epoch 51/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 244ms/step - loss: 24.2787 - mae: 3.6107\n",
      "Epoch 52/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 185ms/step - loss: 25.0274 - mae: 3.5658\n",
      "Epoch 53/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 231ms/step - loss: 26.2972 - mae: 3.6513\n",
      "Epoch 54/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 243ms/step - loss: 40.9958 - mae: 4.3459\n",
      "Epoch 55/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 41.0560 - mae: 4.2874\n",
      "Epoch 56/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - loss: 42.3398 - mae: 4.3900\n",
      "Epoch 57/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 68.0910 - mae: 5.4104\n",
      "Epoch 58/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 78.6462 - mae: 5.6960\n",
      "Epoch 59/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 240ms/step - loss: 71.1486 - mae: 5.5776\n",
      "Epoch 60/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - loss: 116.5912 - mae: 7.1959\n",
      "Epoch 61/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 316ms/step - loss: 133.5539 - mae: 8.2278\n",
      "Epoch 62/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 302ms/step - loss: 148.4426 - mae: 8.7421\n",
      "Epoch 63/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 305ms/step - loss: 175.8898 - mae: 9.5401\n",
      "Epoch 64/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 305ms/step - loss: 207.6191 - mae: 10.2187\n",
      "Epoch 65/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 323ms/step - loss: 295.5297 - mae: 12.2565\n",
      "Epoch 66/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 309ms/step - loss: 549.4907 - mae: 16.8578\n",
      "Epoch 67/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 322ms/step - loss: 821.7975 - mae: 20.3792\n",
      "Epoch 68/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 328ms/step - loss: 1102.6572 - mae: 23.0856\n",
      "Epoch 69/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 315ms/step - loss: 1088.3040 - mae: 23.5564\n",
      "Epoch 70/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 366ms/step - loss: 1349.4463 - mae: 26.5909\n",
      "Epoch 71/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 272ms/step - loss: 1254.2632 - mae: 25.9042\n",
      "Epoch 72/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - loss: 927.1053 - mae: 21.3905\n",
      "Epoch 73/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 236ms/step - loss: 781.3108 - mae: 20.8031\n",
      "Epoch 74/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - loss: 919.6091 - mae: 22.5614\n",
      "Epoch 75/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - loss: 685.6970 - mae: 18.6320\n",
      "Epoch 76/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 212ms/step - loss: 507.8326 - mae: 17.0078\n",
      "Epoch 77/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - loss: 409.1123 - mae: 15.2184\n",
      "Epoch 78/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 204ms/step - loss: 450.4288 - mae: 15.2190\n",
      "Epoch 79/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - loss: 401.6437 - mae: 14.6305\n",
      "Epoch 80/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - loss: 300.8939 - mae: 12.6519\n",
      "Epoch 81/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 256ms/step - loss: 312.1682 - mae: 13.2034\n",
      "Epoch 82/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - loss: 185.1734 - mae: 9.9167\n",
      "Epoch 83/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - loss: 231.1405 - mae: 11.2307\n",
      "Epoch 84/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 187.0989 - mae: 10.0225\n",
      "Epoch 85/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 260.7173 - mae: 12.2087\n",
      "Epoch 86/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - loss: 201.1446 - mae: 10.8183\n",
      "Epoch 87/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 237ms/step - loss: 242.9755 - mae: 11.5748\n",
      "Epoch 88/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 307ms/step - loss: 220.4664 - mae: 10.7111\n",
      "Epoch 89/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 264ms/step - loss: 220.7653 - mae: 10.9447\n",
      "Epoch 90/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - loss: 185.5244 - mae: 10.1462\n",
      "Epoch 91/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 280ms/step - loss: 224.2977 - mae: 11.0869\n",
      "Epoch 92/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - loss: 218.7021 - mae: 10.1247\n",
      "Epoch 93/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - loss: 141.5546 - mae: 8.4681\n",
      "Epoch 94/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 279ms/step - loss: 135.5840 - mae: 8.1567\n",
      "Epoch 95/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - loss: 137.8762 - mae: 8.1911\n",
      "Epoch 96/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - loss: 149.3109 - mae: 9.0901\n",
      "Epoch 97/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 280ms/step - loss: 111.3482 - mae: 7.5830\n",
      "Epoch 98/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 328ms/step - loss: 126.3311 - mae: 8.2242\n",
      "Epoch 99/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 298ms/step - loss: 106.1600 - mae: 7.6496\n",
      "Epoch 100/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 214ms/step - loss: 91.8755 - mae: 6.8833\n",
      "Epoch 101/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - loss: 94.7874 - mae: 7.4067\n",
      "Epoch 102/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 158.2614 - mae: 9.1550\n",
      "Epoch 103/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - loss: 112.4772 - mae: 7.8316\n",
      "Epoch 104/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 103.6827 - mae: 7.3278\n",
      "Epoch 105/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - loss: 101.5976 - mae: 7.2822\n",
      "Epoch 106/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 196ms/step - loss: 103.6914 - mae: 7.5243\n",
      "Epoch 107/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - loss: 139.9821 - mae: 8.8346\n",
      "Epoch 108/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - loss: 364.6026 - mae: 14.5480\n",
      "Epoch 109/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 202ms/step - loss: 472.4524 - mae: 16.1163\n",
      "Epoch 110/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - loss: 409.0720 - mae: 15.3910\n",
      "Epoch 111/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - loss: 401.5686 - mae: 15.0036\n",
      "Epoch 112/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 192ms/step - loss: 318.5510 - mae: 13.1705\n",
      "Epoch 113/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 260.5936 - mae: 11.9897\n",
      "Epoch 114/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 272ms/step - loss: 239.9106 - mae: 11.2264\n",
      "Epoch 115/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 260ms/step - loss: 240.2767 - mae: 11.2300\n",
      "Epoch 116/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 334.1924 - mae: 13.6562\n",
      "Epoch 117/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 283ms/step - loss: 357.7283 - mae: 13.9299\n",
      "Epoch 118/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 618.1981 - mae: 18.4101\n",
      "Epoch 119/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step - loss: 508.0723 - mae: 17.0007\n",
      "Epoch 120/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 187ms/step - loss: 476.3436 - mae: 16.3369\n",
      "Epoch 121/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 198ms/step - loss: 513.2461 - mae: 17.0246\n",
      "Epoch 122/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 765.2460 - mae: 20.0424\n",
      "Epoch 123/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 197ms/step - loss: 1108.3069 - mae: 22.7890\n",
      "Epoch 124/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 239ms/step - loss: 756.9448 - mae: 19.8493\n",
      "Epoch 125/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - loss: 735.6776 - mae: 19.0905\n",
      "Epoch 126/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 567.4619 - mae: 17.5277\n",
      "Epoch 127/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 483.0157 - mae: 15.4143\n",
      "Epoch 128/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 251ms/step - loss: 766.1224 - mae: 20.3384\n",
      "Epoch 129/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 255ms/step - loss: 878.2349 - mae: 21.5204\n",
      "Epoch 130/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 238ms/step - loss: 615.9475 - mae: 17.9202\n",
      "Epoch 131/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - loss: 509.5165 - mae: 16.9070\n",
      "Epoch 132/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 467.3988 - mae: 15.9991\n",
      "Epoch 133/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 420.0829 - mae: 15.1247\n",
      "Epoch 134/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 348.4109 - mae: 13.8877\n",
      "Epoch 135/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - loss: 333.4691 - mae: 13.8498\n",
      "Epoch 136/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - loss: 353.6488 - mae: 14.0193\n",
      "Epoch 137/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 208ms/step - loss: 333.6446 - mae: 13.1082\n",
      "Epoch 138/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - loss: 317.7877 - mae: 13.4826\n",
      "Epoch 139/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 213ms/step - loss: 271.0485 - mae: 12.4471\n",
      "Epoch 140/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 221ms/step - loss: 311.3447 - mae: 13.0512\n",
      "Epoch 141/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 209ms/step - loss: 318.2256 - mae: 13.0492\n",
      "Epoch 142/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 214ms/step - loss: 348.0371 - mae: 13.7772\n",
      "Epoch 143/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 218ms/step - loss: 264.5250 - mae: 12.1644\n",
      "Epoch 144/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 230ms/step - loss: 240.1095 - mae: 11.5573\n",
      "Epoch 145/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 205ms/step - loss: 207.4844 - mae: 10.4361\n",
      "Epoch 146/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - loss: 247.7211 - mae: 11.8612\n",
      "Epoch 147/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - loss: 232.3462 - mae: 11.1041\n",
      "Epoch 148/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 214.0376 - mae: 11.1082\n",
      "Epoch 149/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 293ms/step - loss: 225.9373 - mae: 10.9987\n",
      "Epoch 150/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 260ms/step - loss: 263.0975 - mae: 12.4823\n",
      "Epoch 151/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 224ms/step - loss: 228.5415 - mae: 10.8162\n",
      "Epoch 152/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 257ms/step - loss: 223.2408 - mae: 10.5916\n",
      "Epoch 153/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 224.7884 - mae: 11.0490\n",
      "Epoch 154/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 247ms/step - loss: 256.7208 - mae: 11.3351\n",
      "Epoch 155/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 234ms/step - loss: 302.2120 - mae: 13.3112\n",
      "Epoch 156/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 226ms/step - loss: 226.7363 - mae: 11.3023\n",
      "Epoch 157/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - loss: 227.6895 - mae: 11.1952\n",
      "Epoch 158/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - loss: 208.9453 - mae: 10.8668\n",
      "Epoch 159/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 216ms/step - loss: 181.7535 - mae: 9.8560\n",
      "Epoch 160/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - loss: 228.8020 - mae: 11.5367\n",
      "Epoch 161/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 207ms/step - loss: 280.4352 - mae: 12.5222\n",
      "Epoch 162/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 242ms/step - loss: 515.8132 - mae: 16.9559\n",
      "Epoch 163/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 272ms/step - loss: 639.0084 - mae: 19.1585\n",
      "Epoch 164/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 254ms/step - loss: 796.8138 - mae: 21.4224\n",
      "Epoch 165/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 623.0944 - mae: 18.2169\n",
      "Epoch 166/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 188ms/step - loss: 358.0813 - mae: 14.0806\n",
      "Epoch 167/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 249ms/step - loss: 358.8067 - mae: 14.0150\n",
      "Epoch 168/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - loss: 463.6765 - mae: 16.0673\n",
      "Epoch 169/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 194ms/step - loss: 536.2629 - mae: 16.9074\n",
      "Epoch 170/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 525.9879 - mae: 16.0116\n",
      "Epoch 171/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - loss: 389.7251 - mae: 14.4597\n",
      "Epoch 172/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 209ms/step - loss: 555.6926 - mae: 17.4736\n",
      "Epoch 173/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 251ms/step - loss: 634.1501 - mae: 18.1933\n",
      "Epoch 174/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 282ms/step - loss: 581.0204 - mae: 17.8446\n",
      "Epoch 175/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 249ms/step - loss: 818.7828 - mae: 21.5800\n",
      "Epoch 176/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 228ms/step - loss: 1382.5624 - mae: 28.0698\n",
      "Epoch 177/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 190ms/step - loss: 2463.9702 - mae: 37.0444\n",
      "Epoch 178/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - loss: 2529.4897 - mae: 37.3062\n",
      "Epoch 179/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - loss: 1540.2384 - mae: 29.0516\n",
      "Epoch 180/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 215ms/step - loss: 2945.0994 - mae: 41.3237\n",
      "Epoch 181/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - loss: 1702.4758 - mae: 30.0363\n",
      "Epoch 182/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 212ms/step - loss: 1453.3232 - mae: 27.5429\n",
      "Epoch 183/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 225ms/step - loss: 1758.2234 - mae: 30.3570\n",
      "Epoch 184/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 235ms/step - loss: 1179.0865 - mae: 25.9052\n",
      "Epoch 185/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 219ms/step - loss: 827.7039 - mae: 21.3154\n",
      "Epoch 186/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 237ms/step - loss: 663.7582 - mae: 18.7868\n",
      "Epoch 187/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 252ms/step - loss: 706.6295 - mae: 19.6072\n",
      "Epoch 188/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 338ms/step - loss: 645.0547 - mae: 18.8630\n",
      "Epoch 189/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 288ms/step - loss: 372.2263 - mae: 14.0327\n",
      "Epoch 190/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 281ms/step - loss: 217.1070 - mae: 11.1410\n",
      "Epoch 191/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 276ms/step - loss: 147.4841 - mae: 9.1917\n",
      "Epoch 192/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - loss: 86.3954 - mae: 6.7802\n",
      "Epoch 193/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 270ms/step - loss: 62.2786 - mae: 5.7112\n",
      "Epoch 194/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - loss: 33.0215 - mae: 4.3164\n",
      "Epoch 195/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 271ms/step - loss: 23.7519 - mae: 3.6840\n",
      "Epoch 196/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 270ms/step - loss: 24.5917 - mae: 3.6085\n",
      "Epoch 197/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 273ms/step - loss: 17.0670 - mae: 3.1074\n",
      "Epoch 198/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 281ms/step - loss: 13.0182 - mae: 2.6513\n",
      "Epoch 199/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 274ms/step - loss: 9.0241 - mae: 2.2544\n",
      "Epoch 200/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 267ms/step - loss: 6.9589 - mae: 1.9557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f8a7725930>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "                                    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                                    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                                    tf.keras.layers.Dense(4)\n",
    "                                    ])\n",
    "\n",
    "model.compile(\n",
    "                optimizer='adam', \n",
    "                loss='mean_squared_error', \n",
    "                metrics=['mae']\n",
    "                )\n",
    "model.fit(\n",
    "        X, Y, \n",
    "        epochs=200\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_face_detection(img_path):\n",
    "    face_objs = DeepFace.represent(\n",
    "                                img_path = img_path,\n",
    "                                model_name = \"Facenet512\",\n",
    "                                enforce_detection = False\n",
    "                                )\n",
    "    for i in range(len(face_objs)):\n",
    "        facial_area = face_objs[i]['facial_area']\n",
    "        face_confidence = face_objs[i]['face_confidence']\n",
    "        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle([x, y, x+w, y+h], outline='red', width=5)\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "        draw.text((x-30, y-40), f'Confidence: {face_confidence}', font=font, fill='blue')\n",
    "        \n",
    "    plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize_face_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/facedb/Dilshan Madushanka/dilshan (4).jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvisualize_face_detection\u001b[49m(img_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'visualize_face_detection' is not defined"
     ]
    }
   ],
   "source": [
    "img_path = \"data/facedb/Dilshan Madushanka/dilshan (4).jpg\"\n",
    "visualize_face_detection(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_information_for_db(img_path):\n",
    "    # face_objs = DeepFace.represent(\n",
    "    #                                 img_path = img_path, \n",
    "    #                                 detector_backend = backends[4],\n",
    "    #                                 align = alignment_modes[0],\n",
    "    #                                 enforce_detection = False\n",
    "    #                                 )\n",
    "    face_objs = DeepFace.represent(\n",
    "                                img_path = img_path,\n",
    "                                model_name = models[2],\n",
    "                                enforce_detection = False\n",
    "                                )\n",
    "    img_path = img_path.replace(\"\\\\\", \"/\")\n",
    "    user_name = img_path.split(\"/\")[-2]\n",
    "\n",
    "    if len(face_objs) != 1:\n",
    "        if len(face_objs) == 0:\n",
    "            Warning(f\"No faces detected in the image : {img_path}\")\n",
    "        else:\n",
    "            Warning(f\"Multiple faces detected in the image : {img_path}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    else:\n",
    "        facial_area = face_objs[0]['facial_area']\n",
    "        embeddings = face_objs[0]['embedding']\n",
    "        face_confidence = face_objs[0]['face_confidence']\n",
    "        x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "\n",
    "    return embeddings, face_confidence, (x, y, w, h), user_name\n",
    "\n",
    "def extract_face_information_for_inference(img_path):\n",
    "    face_objs = DeepFace.represent(\n",
    "                                img_path = img_path,\n",
    "                                model_name = models[2],\n",
    "                                enforce_detection = False\n",
    "                                )\n",
    "    img_path = img_path.replace(\"\\\\\", \"/\")\n",
    "\n",
    "    embeddings = []\n",
    "    facial_areas = []\n",
    "    face_confidences = []\n",
    "\n",
    "    if len(face_objs) == 0:\n",
    "        Warning(f\"No faces detected in the image : {img_path}\")\n",
    "    else:\n",
    "        for i in range(len(face_objs)):\n",
    "            embs = face_objs[i]['embedding']\n",
    "            facial_area = face_objs[i]['facial_area']\n",
    "            face_confidence = face_objs[i]['face_confidence']\n",
    "            x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']\n",
    "\n",
    "            embeddings.append(embs)\n",
    "            facial_areas.append((x, y, w, h))   \n",
    "            face_confidences.append(face_confidence)\n",
    "\n",
    "    return embeddings, face_confidences, facial_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_face_embedding_index(\n",
    "                                d = 512,\n",
    "                                face_index_path = 'models/face_index',\n",
    "                                face_image_dir = 'data/facedb/*/*.jpg',\n",
    "                                face_details_path = 'models/face_details.npz',\n",
    "                                ):\n",
    "    if (not os.path.exists(face_index_path)) or (not os.path.exists(face_details_path)):\n",
    "        faiss_index = faiss.index_factory(d, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "        embeddings = []\n",
    "        user_names = []\n",
    "        facial_areas = []\n",
    "        face_confidences = []\n",
    "        \n",
    "        for idx, img_path in enumerate(glob.glob(face_image_dir)):\n",
    "            emb, face_confidence, facial_area, user_name = extract_face_information_for_db(img_path)\n",
    "            if emb is not None:\n",
    "                embeddings.append(emb)\n",
    "                user_names.append(user_name)\n",
    "                facial_areas.append(facial_area)\n",
    "                face_confidences.append(face_confidence)\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Processed {idx}/{len(glob.glob(face_image_dir))} images\")\n",
    "\n",
    "        embeddings = np.asarray(embeddings).astype('float32')\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        faiss_index.add(embeddings)\n",
    "        faiss.write_index(faiss_index, face_index_path)\n",
    "\n",
    "        np.savez(\n",
    "                face_details_path, \n",
    "                user_names=user_names, \n",
    "                facial_areas=facial_areas, \n",
    "                face_confidences=face_confidences\n",
    "                )\n",
    "        \n",
    "    else:\n",
    "        faiss_index = faiss.read_index(face_index_path)\n",
    "        face_details = np.load(face_details_path)\n",
    "        user_names = face_details['user_names']\n",
    "        facial_areas = face_details['facial_areas']\n",
    "        face_confidences = face_details['face_confidences']\n",
    "\n",
    "    return faiss_index, user_names, facial_areas, face_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, user_names, facial_areas, face_confidences = build_face_embedding_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_to_db(\n",
    "                    username, \n",
    "                    face_index_path = 'models/face_index',\n",
    "                    face_details_path = 'models/face_details.npz',\n",
    "                    face_image_dir = 'data/facedb/{}/*.jpg'\n",
    "                    ):\n",
    "    index, user_names, facial_areas, face_confidences = build_face_embedding_index(\n",
    "                                                                                    face_index_path = face_index_path,\n",
    "                                                                                    face_details_path = face_details_path,\n",
    "                                                                                    )\n",
    "    if username in user_names:\n",
    "        Warning(f\"User {username} already exists in the database\")\n",
    "        return\n",
    "    \n",
    "    user_names_to_add = []\n",
    "    facial_areas_to_add = []\n",
    "    face_confidences_to_add = []\n",
    "\n",
    "    for img_path in glob.glob(face_image_dir.format(username)):\n",
    "        emb, face_confidence, facial_area, user_name = extract_face_information_for_db(img_path)\n",
    "        if emb is not None:\n",
    "            embeddings = np.array([emb]).astype('float32')\n",
    "            faiss.normalize_L2(embeddings)\n",
    "            index.add(embeddings)\n",
    "            user_names_to_add.append(user_name)\n",
    "            facial_areas_to_add.append(facial_area)\n",
    "            face_confidences_to_add.append(face_confidence)\n",
    "\n",
    "    faiss.write_index(index, face_index_path)\n",
    "\n",
    "    if len(user_names_to_add) > 0:\n",
    "        user_names = np.concatenate([user_names, user_names_to_add])\n",
    "        facial_areas = np.concatenate([facial_areas, facial_areas_to_add])\n",
    "        face_confidences = np.concatenate([face_confidences, face_confidences_to_add])\n",
    "\n",
    "    np.savez(\n",
    "            face_details_path, \n",
    "            user_names=user_names, \n",
    "            facial_areas=facial_areas, \n",
    "            face_confidences=face_confidences\n",
    "            )\n",
    "    \n",
    "    print(f\"User : {username} added to the database\")\n",
    "\n",
    "def search_face_in_db(\n",
    "                    img_path, \n",
    "                    face_index_path = 'models/face_index',\n",
    "                    face_details_path = 'models/face_details.npz',\n",
    "                    ):\n",
    "    index, user_names, _, _ = build_face_embedding_index(\n",
    "                                                        face_index_path = face_index_path,\n",
    "                                                        face_details_path = face_details_path,\n",
    "                                                        )\n",
    "    embeddings, face_confidences, facial_areas = extract_face_information_for_inference(img_path)\n",
    "\n",
    "    retrieved_user_names = []\n",
    "    retrieved_facial_areas = []\n",
    "    retrieved_face_confidences = []\n",
    "\n",
    "    if embeddings is not None:\n",
    "        for idx, emb in enumerate(embeddings):\n",
    "            if face_confidences[idx] >= 0.8:\n",
    "                emb = np.array(emb).reshape(1, -1).astype('float32')\n",
    "                faiss.normalize_L2(emb)\n",
    "                D, I = index.search(emb, 5)\n",
    "                I = np.array(I).squeeze()\n",
    "                D = np.array(D).squeeze()\n",
    "                user_name_list = [user_names[i] for i in I]\n",
    "                user_name = max(set(user_name_list), key = user_name_list.count)\n",
    "                avg_confidence = np.mean([d for i, d in zip(I, D) if user_names[i] == user_name])\n",
    "                retrieved_face_confidences.append(np.round(avg_confidence, 3))\n",
    "                retrieved_facial_areas.append(facial_areas[idx])\n",
    "                retrieved_user_names.append(user_name)\n",
    "\n",
    "    return retrieved_user_names, retrieved_facial_areas, retrieved_face_confidences\n",
    "\n",
    "def verify_and_visualize_user(  \n",
    "                                username,\n",
    "                                img_path,\n",
    "                                ):\n",
    "    retrieved_user_names, retrieved_facial_areas, retrieved_face_confidences = search_face_in_db(img_path)\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for i in range(len(retrieved_user_names)):\n",
    "        x, y, w, h = retrieved_facial_areas[i]\n",
    "        face_area = w * h\n",
    "        if (face_area >= 20000):\n",
    "            if (retrieved_user_names[i] == username) and (retrieved_face_confidences[i] >= 0.5):\n",
    "                draw.rectangle([x, y, x+w, y+h], outline='green', width=5)\n",
    "                font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "                draw.text((x-30, y-70), f'User: {retrieved_user_names[i]}', font=font, fill='green')\n",
    "            else:\n",
    "                draw.rectangle([x, y, x+w, y+h], outline='red', width=5)\n",
    "                font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def verify_and_visualize_user_realtime(username):\n",
    "    cap = cv.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv.imwrite(\"data/temp_dir/temp.jpg\", frame)\n",
    "        \n",
    "        retrieved_user_names, retrieved_facial_areas, retrieved_face_confidences = search_face_in_db(\"data/temp_dir/temp.jpg\")\n",
    "        for i in range(len(retrieved_user_names)):\n",
    "            x, y, w, h = retrieved_facial_areas[i]\n",
    "            face_area = w * h\n",
    "            if (face_area >= 20000):\n",
    "                if (retrieved_user_names[i] == username) and (retrieved_face_confidences[i] >= 0.5):\n",
    "                    cv.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                    font = cv.FONT_HERSHEY_SIMPLEX\n",
    "                    cv.putText(frame, f'User: {retrieved_user_names[i]}', (x-30, y-40), font, 1, (0, 255, 0), 2)\n",
    "                else:\n",
    "                    cv.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "\n",
    "        cv.imshow('frame', frame)\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : Dilshan Madushanka added to the database\n"
     ]
    }
   ],
   "source": [
    "add_user_to_db('Dilshan Madushanka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve User Names : ['Bubu', 'Alia Bhatt', 'Amitabh Bachchan', 'Amitabh Bachchan']\n",
      "Retrieve Face Confidences : [0.342, 0.702, 0.739, 0.575]\n",
      "Retrieve Facial Areas : [(1035, 801, 1198, 1198), (2301, 2517, 45, 45), (944, 2296, 106, 106), (773, 3197, 62, 62)]\n"
     ]
    }
   ],
   "source": [
    "retrieved_user_names, retrieved_facial_areas, retrieved_face_confidences = search_face_in_db(img_path)\n",
    "\n",
    "print(f\"Retrieve User Names : {retrieved_user_names}\")\n",
    "print(f\"Retrieve Face Confidences : {retrieved_face_confidences}\")\n",
    "print(f\"Retrieve Facial Areas : {retrieved_facial_areas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Confirm that data/test_images/dilshan (4).jpg exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mverify_and_visualize_user\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDilshan Madushanka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/test_images/dilshan (4).jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 81\u001b[0m, in \u001b[0;36mverify_and_visualize_user\u001b[1;34m(username, img_path)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mverify_and_visualize_user\u001b[39m(  \n\u001b[0;32m     78\u001b[0m                                 username,\n\u001b[0;32m     79\u001b[0m                                 img_path,\n\u001b[0;32m     80\u001b[0m                                 ):\n\u001b[1;32m---> 81\u001b[0m     retrieved_user_names, retrieved_facial_areas, retrieved_face_confidences \u001b[38;5;241m=\u001b[39m \u001b[43msearch_face_in_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m     84\u001b[0m     draw \u001b[38;5;241m=\u001b[39m ImageDraw\u001b[38;5;241m.\u001b[39mDraw(image)\n",
      "Cell \u001b[1;32mIn[13], line 54\u001b[0m, in \u001b[0;36msearch_face_in_db\u001b[1;34m(img_path, face_index_path, face_details_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch_face_in_db\u001b[39m(\n\u001b[0;32m     46\u001b[0m                     img_path, \n\u001b[0;32m     47\u001b[0m                     face_index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/face_index\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m                     face_details_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/face_details.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     49\u001b[0m                     ):\n\u001b[0;32m     50\u001b[0m     index, user_names, _, _ \u001b[38;5;241m=\u001b[39m build_face_embedding_index(\n\u001b[0;32m     51\u001b[0m                                                         face_index_path \u001b[38;5;241m=\u001b[39m face_index_path,\n\u001b[0;32m     52\u001b[0m                                                         face_details_path \u001b[38;5;241m=\u001b[39m face_details_path,\n\u001b[0;32m     53\u001b[0m                                                         )\n\u001b[1;32m---> 54\u001b[0m     embeddings, face_confidences, facial_areas \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face_information_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     retrieved_user_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     57\u001b[0m     retrieved_facial_areas \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m, in \u001b[0;36mextract_face_information_for_inference\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_face_information_for_inference\u001b[39m(img_path):\n\u001b[1;32m---> 32\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m img_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\DeepFace.py:418\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent\u001b[39m(\n\u001b[0;32m    360\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    361\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG-Face\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m     max_faces: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    369\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    Represent facial images as multi-dimensional vector embeddings.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            to 'skip', the confidence will be 0 and is nonsensical.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepresentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\modules\\representation.py:76\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[0m\n\u001b[0;32m     74\u001b[0m target_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput_shape\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detector_backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     img_objs \u001b[38;5;241m=\u001b[39m \u001b[43mdetection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# skip\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Try load. If load error, will raise exception internal\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     img, _ \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mload_image(img_path)\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\modules\\detection.py:83\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, detector_backend, enforce_detection, align, expand_percentage, grayscale, color_face, normalize_face, anti_spoofing)\u001b[0m\n\u001b[0;32m     80\u001b[0m resp_objs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# img might be path, base64 or numpy array. Convert it to numpy whatever it is.\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m img, img_name \u001b[38;5;241m=\u001b[39m \u001b[43mimage_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\commons\\image_utils.py:93\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# The image is a path\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(img):\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfirm that \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# image must be a file on the system then\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# image name must have english characters\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m img\u001b[38;5;241m.\u001b[39misascii():\n",
      "\u001b[1;31mValueError\u001b[0m: Confirm that data/test_images/dilshan (4).jpg exists"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "verify_and_visualize_user(\"Dilshan Madushanka\", 'data/test_images/dilshan (4).jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mverify_and_visualize_user_realtime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDilshan Madushanka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 110\u001b[0m, in \u001b[0;36mverify_and_visualize_user_realtime\u001b[1;34m(username)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    108\u001b[0m cv\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/temp_dir/temp.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m--> 110\u001b[0m retrieved_user_names, retrieved_facial_areas, retrieved_face_confidences \u001b[38;5;241m=\u001b[39m \u001b[43msearch_face_in_db\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/temp_dir/temp.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(retrieved_user_names)):\n\u001b[0;32m    112\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m retrieved_facial_areas[i]\n",
      "Cell \u001b[1;32mIn[13], line 54\u001b[0m, in \u001b[0;36msearch_face_in_db\u001b[1;34m(img_path, face_index_path, face_details_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch_face_in_db\u001b[39m(\n\u001b[0;32m     46\u001b[0m                     img_path, \n\u001b[0;32m     47\u001b[0m                     face_index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/face_index\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     48\u001b[0m                     face_details_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/face_details.npz\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     49\u001b[0m                     ):\n\u001b[0;32m     50\u001b[0m     index, user_names, _, _ \u001b[38;5;241m=\u001b[39m build_face_embedding_index(\n\u001b[0;32m     51\u001b[0m                                                         face_index_path \u001b[38;5;241m=\u001b[39m face_index_path,\n\u001b[0;32m     52\u001b[0m                                                         face_details_path \u001b[38;5;241m=\u001b[39m face_details_path,\n\u001b[0;32m     53\u001b[0m                                                         )\n\u001b[1;32m---> 54\u001b[0m     embeddings, face_confidences, facial_areas \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face_information_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     retrieved_user_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     57\u001b[0m     retrieved_facial_areas \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m, in \u001b[0;36mextract_face_information_for_inference\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_face_information_for_inference\u001b[39m(img_path):\n\u001b[1;32m---> 32\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                                \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m img_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\DeepFace.py:418\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrepresent\u001b[39m(\n\u001b[0;32m    360\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    361\u001b[0m     model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG-Face\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m     max_faces: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    369\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    Represent facial images as multi-dimensional vector embeddings.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            to 'skip', the confidence will be 0 and is nonsensical.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepresentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\modules\\representation.py:76\u001b[0m, in \u001b[0;36mrepresent\u001b[1;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[0m\n\u001b[0;32m     74\u001b[0m target_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minput_shape\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detector_backend \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     img_objs \u001b[38;5;241m=\u001b[39m \u001b[43mdetection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# skip\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# Try load. If load error, will raise exception internal\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     img, _ \u001b[38;5;241m=\u001b[39m image_utils\u001b[38;5;241m.\u001b[39mload_image(img_path)\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\modules\\detection.py:95\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, detector_backend, enforce_detection, align, expand_percentage, grayscale, color_face, normalize_face, anti_spoofing)\u001b[0m\n\u001b[0;32m     93\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m [DetectedFace(img\u001b[38;5;241m=\u001b[39mimg, facial_area\u001b[38;5;241m=\u001b[39mbase_region, confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# in case of no face found\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(face_objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m enforce_detection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\modules\\detection.py:234\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(detector_backend, img, align, expand_percentage)\u001b[0m\n\u001b[0;32m    223\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcopyMakeBorder(\n\u001b[0;32m    224\u001b[0m         img,\n\u001b[0;32m    225\u001b[0m         height_border,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m         value\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Color of the border (black)\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# find facial areas of given image\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m facial_areas \u001b[38;5;241m=\u001b[39m \u001b[43mface_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m facial_area \u001b[38;5;129;01min\u001b[39;00m facial_areas:\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\models\\face_detection\\OpenCv.py:55\u001b[0m, in \u001b[0;36mOpenCvClient.detect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h), confidence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(faces, scores):\n\u001b[0;32m     54\u001b[0m     detected_face \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;28mint\u001b[39m(y) : \u001b[38;5;28mint\u001b[39m(y \u001b[38;5;241m+\u001b[39m h), \u001b[38;5;28mint\u001b[39m(x) : \u001b[38;5;28mint\u001b[39m(x \u001b[38;5;241m+\u001b[39m w)]\n\u001b[1;32m---> 55\u001b[0m     left_eye, right_eye \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_eyes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetected_face\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# eyes found in the detected face instead image itself\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# detected face's coordinates should be added\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left_eye \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PurnaLakshitha\\Desktop\\Research\\project\\USER_VARIFICATION_SERVICE\\.myenv3\\lib\\site-packages\\deepface\\models\\face_detection\\OpenCv.py:96\u001b[0m, in \u001b[0;36mOpenCvClient.find_eyes\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m left_eye, right_eye\n\u001b[0;32m     92\u001b[0m detected_face_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(\n\u001b[0;32m     93\u001b[0m     img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY\n\u001b[0;32m     94\u001b[0m )  \u001b[38;5;66;03m# eye detector expects gray scale image\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m eyes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meye_detector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetected_face_gray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# opencv eye detection module is not strong. it might find more than 2 eyes!\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# besides, it returns eyes with different order in each call (issue 435)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# this is an important issue because opencv is the default detector and ssd also uses this\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# find the largest 2 eye. Thanks to @thelostpeace\u001b[39;00m\n\u001b[0;32m    105\u001b[0m eyes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(eyes, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m v: \u001b[38;5;28mabs\u001b[39m(v[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m v[\u001b[38;5;241m3\u001b[39m]), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verify_and_visualize_user_realtime(\"Dilshan Madushanka\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
